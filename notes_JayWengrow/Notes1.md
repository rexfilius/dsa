when we measure how “fast” an operation takes, we do not refer
to how fast the operation takes in terms of pure time, but instead
in how many steps it takes.

Why do we measure code’s speed in terms of steps?
We do this because we can never say definitively that any operation
takes, say, five seconds. While a piece of code may take five
seconds on a particular computer, that same piece of code may
take longer on an older piece of hardware. For that matter, that
same code might run much faster on the supercomputers of tomorrow.
Measuring the speed of an operation in terms of time
is undependable, since the time will always change depending
on the hardware it is run on.

Measuring the speed of an operation is also known as measuring
its time complexity.

An algorithm is simply a set of instructions for completing
a specific task.
